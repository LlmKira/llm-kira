
# ğŸ”¨Attention
llm-kira åº“å› ä¸æ’ä»¶æ ¸å¿ƒå‘½åæœºåˆ¶å†²çªè€Œè¢«åˆ é™¤ã€‚
æ‚¨ä¸èƒ½å†åœ¨æ–°æœºå™¨ä¸Šæ‰§è¡Œ `pip install llm-kira`

è½¬è€Œä»£ä¹‹ï¼Œå¦‚æœæ‚¨æœ‰éœ€è¦ï¼Œå¯ä»¥å…‹éš†é¡¹ç›®ä½¿ç”¨ 
```
git clone https://github.com/LlmKira/llm-kira/tree/0.7.4
pip install poetry
cd llm-kira
poetry install
```

The llm-kira library was removed due to conflict with the plugin core naming mechanism.
You can no longer execute `pip install llm-kira` on a new machine

Instead, if you need, you can clone the project and use
```
git clone https://github.com/LlmKira/llm-kira/tree/0.7.4
pip install poetry
cd llm-kira
poetry install
```
# llm-kira


**è½»é‡çº§å¤šè¯­è¨€æ¨¡å‹å¼‚æ­¥èŠå¤©æœºå™¨äººæ¡†æ¶ã€‚**

## Features

* å…¨å¼‚æ­¥é«˜å¹¶å‘è®¾è®¡
* å°½é‡ç®€å•çš„ API è®¾è®¡
* ç®¡ç†å¯¹è¯æ•°æ®å’Œå‘é‡æ•°æ®

## Basic Use

`pip install -U llm-kira`

**Init**

```python
import llm_kira

llm_kira.setting.redisSetting = llm_kira.setting.RedisConfig(host="localhost",
                                                             port=6379,
                                                             db=0,
                                                             password=None)
llm_kira.setting.dbFile = "client_memory.db"
llm_kira.setting.proxyUrl = None  # "127.0.0.1"

# Plugin
llm_kira.setting.webServerUrlFilter = False
llm_kira.setting.webServerStopSentence = ["å¹¿å‘Š", "è¥é”€å·"]  # æœ‰é»˜è®¤å€¼
```

## Demo

**!! More examples of use in `test/test.py`.**

Take `openai` as an example

```python
import asyncio
import random
import llm_kira
from llm_kira.creator import Optimizer
from llm_kira.types import PromptItem, Interaction
from llm_kira.llms import OpenAiParam
from typing import List

openaiApiKey = ["key1", "key2"]
openaiApiKey: List[str]

receiver = llm_kira.client
conversation = receiver.Conversation(
    start_name="Human:",
    restart_name="AI:",
    conversation_id=10093,  # random.randint(1, 10000000),
)

llm = llm_kira.client.llms.OpenAi(
    profile=conversation,
    api_key=openaiApiKey,
    token_limit=3700,
    auto_penalty=False,
    call_func=None,
)

mem = receiver.MemoryManager(profile=conversation)
chat_client = receiver.ChatBot(profile=conversation,
                               llm_model=llm
                               )


async def chat():
    promptManager = llm_kira.creator.PromptEngine(
        reverse_prompt_buffer=False,  # è®¾å®šæ˜¯é¦–æ¡è¿˜æ˜¯æœ«å°¾çš„ prompt å½“ input
        profile=conversation,
        connect_words="\n",
        memory_manger=mem,
        llm_model=llm,
        description="è¿™æ˜¯ä¸€æ®µå¯¹è¯",  # æ¨èåœ¨è¿™é‡Œè¿›è¡Œå¼ºæ³¨å…¥
        reference_ratio=0.5,
        forget_words=["å¿˜æ‰å¯¹è¯"],
        optimizer=Optimizer.SinglePoint,
    )
    # ç¬¬ä¸‰äººç§°
    promptManager.insert_prompt(prompt=PromptItem(start="Neko", text="å–µå–µå–µ"))
    # ç›´æ¥æ·»åŠ 
    promptManager.insert_interaction(Interaction(single=True, ask=PromptItem(start="Neko", text="MewMewMewMew")))
    # æ·»åŠ äº¤äº’
    promptManager.insert_interaction(Interaction(single=False,
                                                 ask=PromptItem(start="Neko", text="MewMewMewMew"),
                                                 reply=PromptItem(start="Neko", text="MewMewMewMew"))
                                     )
    # æ·»åŠ æ–°å†…å®¹
    promptManager.insert_prompt(prompt=PromptItem(start=conversation.start_name, text=input("TestPrompt:")))
    response = await chat_client.predict(
        prompt=promptManager,
        llm_param=OpenAiParam(model_name="text-davinci-003", temperature=0.8, presence_penalty=0.1, n=1, best_of=1),
        predict_tokens=1000,
    )
    print(f"id {response.conversation_id}")
    print(f"ask {response.ask}")
    print(f"reply {response.reply}")
    print(f"usage:{response.llm.usage}")
    print(f"usage:{response.llm.raw}")
    print(f"---{response.llm.time}---")

    promptManager.clean(clean_prompt=True, clean_knowledge=False, clean_memory=False)
    promptManager.insert_prompt(prompt=PromptItem(start=conversation.start_name, text='ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·'))
    response = await chat_client.predict(llm_param=OpenAiParam(model_name="text-davinci-003"),
                                         prompt=promptManager,
                                         predict_tokens=500,
                                         # parse_reply=None
                                         )
    _info = "parse_reply å‡½æ•°å›è°ƒä¼šå¤„ç† llm çš„å›å¤å­—æ®µï¼Œæ¯”å¦‚ list ç­‰ï¼Œä¼ å…¥listï¼Œä¼ å‡º str çš„å›å¤ã€‚å¿…é¡»æ˜¯ strã€‚"
    _info2 = "The parse_reply function callback handles the reply fields of llm, such as list, etc. Pass in list and pass out str for the reply."
    print(f"id {response.conversation_id}")
    print(f"ask {response.ask}")
    print(f"reply {response.reply}")
    print(f"usage:{response.llm.usage}")
    print(f"usage:{response.llm.raw}")
    print(f"---{response.llm.time}---")


asyncio.run(chat())
```

## Life Status Builder

```python
import llm_kira
from llm_kira.creator.think import ThinkEngine, Hook

conversation = llm_kira.client.Conversation(
    start_name="Human:",
    restart_name="AI:",
    conversation_id=10093,  # random.randint(1, 10000000),
)
_think = ThinkEngine(profile=conversation)
_think.register_hook(Hook(name="happy", trigger="happy", value=2, last=60, time=int(time.time())))  # 60s
# Hook
_think.hook("happy")
print(_think.hook_pool)
print(_think.build_status(rank=5))
# rank=sum(value,value,value)
```

## Frame

```
â”œâ”€â”€ client
â”‚        â”œâ”€â”€ agent.py // åŸºæœ¬ç±»
â”‚        â”œâ”€â”€ anchor.py // ä»£ç†ç«¯
â”‚        â”œâ”€â”€ enhance.py // å¤–éƒ¨æ¥å£æ–¹æ³•
â”‚        â”œâ”€â”€ __init__.py 
â”‚        â”œâ”€â”€ llms  // å¤§è¯­è¨€æ¨¡å‹ç±»
â”‚        â”œâ”€â”€ module // æ³¨å…¥æ¨¡ç»„
â”‚        â”œâ”€â”€ Optimizer.py  // ä¼˜åŒ–å™¨
â”‚        â”œâ”€â”€ test //æµ‹è¯•
â”‚        â”œâ”€â”€ text_analysis_tools
â”‚        â”œâ”€â”€ types.py // ç±»å‹
â”‚        â””â”€â”€ vocab.json 
â”œâ”€â”€ creator  // æç¤ºæ„å»ºå¼•æ“
â”‚        â”œâ”€â”€ engine.py
â”‚        â”œâ”€â”€ __init__.py
â”œâ”€â”€ error.py // é€šç”¨é”™è¯¯ç±»å‹
â”œâ”€â”€ __init__.py //ä¸»å…¥å£
â”œâ”€â”€ radio    // å¤–éƒ¨é€šä¿¡ç±»å‹ï¼ŒçŸ¥è¯†æ± 
â”‚        â”œâ”€â”€ anchor.py
â”‚        â”œâ”€â”€ crawer.py
â”‚        â”œâ”€â”€ decomposer.py
â”‚        â””â”€â”€ setting.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ tool  // LLM å·¥å…·ç±»å‹
â”‚        â”œâ”€â”€ __init__.py
â”‚        â”œâ”€â”€ openai
â””â”€â”€ utils // å·¥å…·ç±»å‹/è¯­è¨€æ¢æµ‹
    â”œâ”€â”€ chat.py
    â”œâ”€â”€ data.py
    â”œâ”€â”€ fatlangdetect
    â”œâ”€â”€ langdetect
    â”œâ”€â”€ network.py
    â””â”€â”€ setting.py
```
